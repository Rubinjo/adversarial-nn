# adversarial-nn
![Python](https://img.shields.io/badge/python-3670A0?style=flat-square&logo=python&logoColor=ffdd54)
![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=flat-square&logo=anaconda&logoColor=white)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=flat-square&logo=jupyter&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat-square&logo=PyTorch&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat-square&logo=numpy&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat-square&logo=scikit-learn&logoColor=white)

This project was executed as a school assignment at the University of Twente. In this project a basic CNN model (created by ourselves) and the ResNet-50 model are trained upon the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset whereafter different adversial attacks and defences are applied to look at their impact on the classification accuracy.

## Project Overview

- School: University of Twente
- Course: Deep Learning - From Theory to Practice
- Assignment Type: Topic wiht open implementation
- Group Size: 4

## Execution

All code with explanation can be found in the `main.ipynb` notebook. Trained models are saved in the `model` directory and figures are saved in the `results` dictionary.
